import streamlit as st
import os

st.set_page_config(page_title="Methodology", page_icon="ðŸ› ", layout="wide")

st.title("ðŸ›  Methodology")

st.markdown("""
This application uses **Retrievalâ€‘Augmented Generation (RAG)**. At startup it loads PDFs, extracts text, chunks, embeds, and builds a local, fileâ€‘based vector index. Queries then retrieve top matches for grounded answers.

### Data Flow
1. **Load** â€“ PDFs from `data/guides/` and any uploads from `data/uploads/`.
2. **Extract** â€“ Text extracted with `pypdf`.
3. **Chunk** â€“ Text split into overlapping sections for better recall.
4. **Embed** â€“ OpenAI embeddings for each chunk.
5. **Index** â€“ Stored in Chroma (persistent) under `vectorstore/`.
6. **Query** â€“ User question/keywords embedded and matched via cosine similarity.
7. **Answer** â€“ For Case 1, the LLM forms a response using retrieved context; for Case 2, snippets are shown directly.
""")

st.subheader("Case 1 â€” Chat with Preloaded Guides")
case1_path = "assets/flowcharts/case1_flowchart.png"
if os.path.exists(case1_path):
    st.image(case1_path, caption="Flowchart â€” Case 1", use_container_width=True)
else:
    st.info(f"Add flowchart image at: {case1_path}")

st.subheader("Case 2 â€” Intelligent Search")
case2_path = "assets/flowcharts/case2_flowchart.png"
if os.path.exists(case2_path):
    st.image(case2_path, caption="Flowchart â€” Case 2", use_container_width=True)
else:
    st.info(f"Add flowchart image at: {case2_path}")
